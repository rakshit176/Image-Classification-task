{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc6acbd-59b6-4abc-8f65-a2b4d0a9c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator # Try to generate multiple data from single images\n",
    "from keras.models import Sequential # For modeling neural network we can have object of Sequential which we can call model\n",
    "from keras.layers import Conv2D, MaxPooling2D # conventional neural networks for extract features from images , Pooling helps in reduce size of images or data\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense # Activation to tell when to be at 1 ,0 or -1 , Dropout need so that our model does not overfit, Flatten converts 2d images to 1d array size, Dense used to create hidden layers or output layers\n",
    "from keras import backend as K # Helps in understanding channels\n",
    "import numpy as np # manipulate arrays in wise and in better and optimize form\n",
    "from keras.preprocessing import image #import images from library and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e40906-cd7e-406a-840f-e08aa0b0443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150,150 # dimension of  images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85f8a5c-00b7-4ce7-9a18-2b084957de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'D:/indian actors/train' # train dataset\n",
    "test_data_dir = 'D:/indian actors/test' # test dataset\n",
    "num_train_samples = 300 # number of train samples used from train data\n",
    "num_test_samples = 100 # number of test samples used from test data\n",
    "epochs = 50 # no of times sending batch of data again and again same network\n",
    "batch_size = 20 # batching of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6077050d-4142-455a-8637-a5bf6f610f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check images are in right format \n",
    "if K.image_data_format() == 'channels_first' :\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3) # 150,150,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40b469b-1db4-432d-a19b-586081768c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mulitiple data from train data (ex: Now we have 1000 we can use below codes to generate 4000 images in different angles)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255,  # rescaling of images\n",
    "    shear_range = 0.2,   # shear the iamge twisting of images\n",
    "    zoom_range = 0.2,   # zooming of images\n",
    "    horizontal_flip = True) # horizontal flip of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63e18ffd-a6b2-4698-839b-78197b03b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale = 1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c5e1aa9-ae22-4d8e-bf33-f9dbb127820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 692 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Importing all the data images from train dataset\n",
    "train_generate = train_datagen.flow_from_directory(\n",
    "     train_data_dir,\n",
    "     target_size = (img_width, img_height),\n",
    "     batch_size = batch_size,\n",
    "     class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb0fd3b8-ced2-4f1f-8b8e-59051caaaf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Importing all the data images from test dataset\n",
    "test_generate = test_datagen.flow_from_directory(\n",
    "     test_data_dir,\n",
    "     target_size = (img_width, img_height),\n",
    "     batch_size = batch_size,\n",
    "     class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "534e4863-3377-483d-be52-06beffac4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "=================================================================\n",
      "Total params: 896\n",
      "Trainable params: 896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating Conventional Neural network model:\n",
    "model = Sequential() #  For modeling neural network we can have object of Sequential which we can call model\n",
    "\n",
    "model.add(Conv2D(32,(3,3), input_shape = input_shape)) # Extract 32 features from the images given and search for features size of serach from metrics is 3 x 3 (serch 3 x 3 pixels and keep iterating over the pixels), inputshape how much shape of data images should go to nerural network \n",
    "model.add(Activation('relu')) # relu is the max function(x,0) with input x e.g. matrix from a convolved image. relu then sets all negative values in the matrix x to zero and all other values are kept constant. relu is computed after the convolution and is a nonlinear activation function like tanh or sigmoid.\n",
    "model.add(MaxPooling2D(pool_size =(2,2))) # To reduce images size whole size to 2 x 2 by mataining same features from data\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3))) # Extract 64 features from the images given and search for features size of serach from metrics is 3 x 3 (serch 3 x 3 pixels and keep iterating over the pixels)\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "\n",
    "model.add(Flatten()) # from 2d image to 1d images\n",
    "model.add(Dense(64)) # create hidden layers which activate data given and gives output (create 64 loops )\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) # does not overfit model\n",
    "model.add(Dense(1)) # all 64 loops are created to be one output\n",
    "model.add(Activation('sigmoid')) # to define or get specific data like 1\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8beb6573-33f9-462f-aedb-48d394b5826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after defining all layers complie as a box  like just provide input and get output\n",
    "model.compile(loss = 'binary_crossentropy',  \n",
    "              optimizer = 'rmsprop',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c019958-ed0d-40bd-8a35-fc75db07089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-88a9ca31561c>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 17s 1s/step - loss: 0.6905 - accuracy: 0.5833 - val_loss: 0.5739 - val_accuracy: 0.8200\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 14s 902ms/step - loss: 0.5878 - accuracy: 0.7500 - val_loss: 0.2802 - val_accuracy: 0.8400\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 9s 614ms/step - loss: 0.6307 - accuracy: 0.8356 - val_loss: 0.2139 - val_accuracy: 0.9900\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 8s 547ms/step - loss: 0.2177 - accuracy: 0.9555 - val_loss: 0.1702 - val_accuracy: 0.9400\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 8s 516ms/step - loss: 0.1868 - accuracy: 0.9349 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 9s 570ms/step - loss: 0.1666 - accuracy: 0.9367 - val_loss: 0.0533 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 8s 528ms/step - loss: 0.0805 - accuracy: 0.9829 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 7s 438ms/step - loss: 0.0534 - accuracy: 0.9867 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 8s 509ms/step - loss: 0.0819 - accuracy: 0.9829 - val_loss: 2.0073 - val_accuracy: 0.6900\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 6s 420ms/step - loss: 0.1196 - accuracy: 0.9726 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 8s 538ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 2.5527e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 7s 464ms/step - loss: 0.3175 - accuracy: 0.9467 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 8s 511ms/step - loss: 0.0319 - accuracy: 0.9967 - val_loss: 8.8931e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 7s 453ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 2.1340e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 8s 532ms/step - loss: 0.1626 - accuracy: 0.9521 - val_loss: 8.6160e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 7s 470ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.8728e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 8s 517ms/step - loss: 0.0333 - accuracy: 0.9867 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 7s 455ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 7s 499ms/step - loss: 0.1264 - accuracy: 0.9760 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 7s 459ms/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 1.5847e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 7s 490ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 3.3975e-05 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 7s 471ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 9.0563e-06 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 7s 486ms/step - loss: 0.0390 - accuracy: 0.9932 - val_loss: 1.1713e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 7s 478ms/step - loss: 7.6500e-04 - accuracy: 1.0000 - val_loss: 4.6657e-06 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 7s 471ms/step - loss: 6.2166e-04 - accuracy: 1.0000 - val_loss: 1.9875e-06 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 7s 490ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 1.0476e-05 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 7s 476ms/step - loss: 2.7361e-04 - accuracy: 1.0000 - val_loss: 1.6042e-07 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 7s 488ms/step - loss: 0.3197 - accuracy: 0.9589 - val_loss: 0.0225 - val_accuracy: 0.9800\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 7s 480ms/step - loss: 0.0111 - accuracy: 0.9967 - val_loss: 2.3996e-05 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 7s 499ms/step - loss: 9.7605e-04 - accuracy: 1.0000 - val_loss: 5.1794e-06 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 7s 482ms/step - loss: 8.3068e-04 - accuracy: 1.0000 - val_loss: 2.8588e-05 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 8s 518ms/step - loss: 1.4154e-04 - accuracy: 1.0000 - val_loss: 1.5145e-07 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 7s 500ms/step - loss: 0.0037 - accuracy: 0.9967 - val_loss: 4.4706e-07 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 8s 553ms/step - loss: 0.0054 - accuracy: 0.9967 - val_loss: 2.6409e-08 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 8s 533ms/step - loss: 4.6228e-04 - accuracy: 1.0000 - val_loss: 8.7850e-09 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 8s 517ms/step - loss: 0.2427 - accuracy: 0.9760 - val_loss: 0.0468 - val_accuracy: 0.9700\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 8s 514ms/step - loss: 0.0061 - accuracy: 0.9967 - val_loss: 1.8710e-06 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 7s 494ms/step - loss: 7.6254e-04 - accuracy: 1.0000 - val_loss: 1.8068e-06 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 9s 625ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.2737e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 7s 481ms/step - loss: 5.3459e-04 - accuracy: 1.0000 - val_loss: 3.8719e-07 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 8s 536ms/step - loss: 2.1100e-04 - accuracy: 1.0000 - val_loss: 2.2736e-06 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 7s 458ms/step - loss: 7.4422e-05 - accuracy: 1.0000 - val_loss: 3.0042e-07 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 8s 541ms/step - loss: 4.7759e-05 - accuracy: 1.0000 - val_loss: 3.1169e-09 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 7s 467ms/step - loss: 2.1054e-05 - accuracy: 1.0000 - val_loss: 3.8028e-09 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 8s 539ms/step - loss: 0.5553 - accuracy: 0.9633 - val_loss: 1.9656e-07 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 7s 465ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 5.9251e-08 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 8s 515ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3381e-07 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 7s 459ms/step - loss: 5.9862e-04 - accuracy: 1.0000 - val_loss: 2.3619e-07 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 8s 537ms/step - loss: 4.0133e-04 - accuracy: 1.0000 - val_loss: 8.4647e-07 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 7s 482ms/step - loss: 0.0987 - accuracy: 0.9800 - val_loss: 0.1117 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e7f11c11c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the augmentation configuration we use for training\n",
    "model.fit_generator(\n",
    "      train_generate,\n",
    "      steps_per_epoch = num_train_samples // batch_size,\n",
    "      epochs = epochs,\n",
    "      validation_data = test_generate,\n",
    "      validation_steps = num_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a335b960-3bcf-4d84-bcf8-119804c31921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to perform the model in hardware or any prediction\n",
    "model.save_weights('D:/indian actors/model2/trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26ddd9c3-3b38-4426-b27b-014d2967aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image for prediction from test data\n",
    "img_pred = image.load_img('D:/indian actors/test/Shraddha_Kapoor/Shraddha_Kapoor_2.jpg', target_size= (150,150)) # loading images (specfying a Shraddha_Kapoor image from test data) and target size of 150 x 150 \n",
    "img_pred = image.img_to_array(img_pred) # image array : convert of image to array type\n",
    "img_pred = np.expand_dims(img_pred, axis = 0) # expand dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a657352-9e62-43e6-b3c3-ee709bee2b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('D:/indian actors/model2/trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a9d9656-7b40-4e53-9874-0e06f8dfb653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8784839]]\n",
      "Shraddha_Kapoor\n"
     ]
    }
   ],
   "source": [
    "# prediction of result\n",
    "result = model.predict(img_pred) # predict the array img using model\n",
    "print(result) # print result\n",
    "if result[0][0] == 0: # if given 2d array is 0 then it is Hrithik_Roshan else Shraddha_Kapoor\n",
    "    prediction = \"Hrithik_Roshan\"\n",
    "else :\n",
    "    prediction = \"Shraddha_Kapoor\"\n",
    "    \n",
    "    \n",
    "print (prediction)  # print prediction of images   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3750996-723b-4b95-af73-c2510ebfca5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
