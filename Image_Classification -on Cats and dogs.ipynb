{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc6acbd-59b6-4abc-8f65-a2b4d0a9c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator # Try to generate multiple data from single images\n",
    "from keras.models import Sequential # For modeling neural network we can have object of Sequential which we can call model\n",
    "from keras.layers import Conv2D, MaxPooling2D # conventional neural networks for extract features from images , Pooling helps in reduce size of images or data\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense # Activation to tell when to be at 1 ,0 or -1 , Dropout need so that our model does not overfit, Flatten converts 2d images to 1d array size, Dense used to create hidden layers or output layers\n",
    "from keras import backend as K # Helps in understanding channels\n",
    "import numpy as np # manipulate arrays in wise and in better and optimize form\n",
    "from keras.preprocessing import image #import images from library and process them\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3e40906-cd7e-406a-840f-e08aa0b0443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 150,150 # dimension of  images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a85f8a5c-00b7-4ce7-9a18-2b084957de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'D:/Cats and Dogs/dataset/training_set' # train dataset\n",
    "test_data_dir = 'D:/Cats and Dogs/dataset/test_set' # test dataset\n",
    "num_train_samples = 1000 # number of train samples used from train data\n",
    "num_test_samples = 100 # number of test samples used from test data\n",
    "epochs = 50 # no of times sending batch of data again and again same network\n",
    "batch_size = 20 # batching of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6077050d-4142-455a-8637-a5bf6f610f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check images are in right format \n",
    "if K.image_data_format() == 'channels_first' :\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3) # 150,150,3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d40b469b-1db4-432d-a19b-586081768c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mulitiple data from train data (ex: Now we have 1000 we can use below codes to generate 4000 images in different angles)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1. / 255,  # rescaling of images\n",
    "    shear_range = 0.2,   # shear the iamge twisting of images\n",
    "    zoom_range = 0.2,   # zooming of images\n",
    "    horizontal_flip = True) # horizontal flip of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63e18ffd-a6b2-4698-839b-78197b03b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale = 1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c5e1aa9-ae22-4d8e-bf33-f9dbb127820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Importing all the data images from train dataset\n",
    "train_generate = train_datagen.flow_from_directory(\n",
    "     train_data_dir,\n",
    "     target_size = (img_width, img_height),\n",
    "     batch_size = batch_size,\n",
    "     class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb0fd3b8-ced2-4f1f-8b8e-59051caaaf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Importing all the data images from test dataset\n",
    "test_generate = test_datagen.flow_from_directory(\n",
    "     test_data_dir,\n",
    "     target_size = (img_width, img_height),\n",
    "     batch_size = batch_size,\n",
    "     class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "534e4863-3377-483d-be52-06beffac4b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "=================================================================\n",
      "Total params: 896\n",
      "Trainable params: 896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 34, 34, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18496)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1183808   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,212,513\n",
      "Trainable params: 1,212,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating Conventional Neural network model:\n",
    "model = Sequential() #  For modeling neural network we can have object of Sequential which we can call model\n",
    "\n",
    "model.add(Conv2D(32,(3,3), input_shape = input_shape)) # Extract 32 features from the images given and search for features size of serach from metrics is 3 x 3 (serch 3 x 3 pixels and keep iterating over the pixels), inputshape how much shape of data images should go to nerural network \n",
    "model.add(Activation('relu')) # relu is the max function(x,0) with input x e.g. matrix from a convolved image. relu then sets all negative values in the matrix x to zero and all other values are kept constant. relu is computed after the convolution and is a nonlinear activation function like tanh or sigmoid.\n",
    "model.add(MaxPooling2D(pool_size =(2,2))) # To reduce images size whole size to 2 x 2 by mataining same features from data\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.add(Conv2D(32,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3))) # Extract 64 features from the images given and search for features size of serach from metrics is 3 x 3 (serch 3 x 3 pixels and keep iterating over the pixels)\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "\n",
    "model.add(Flatten()) # from 2d image to 1d images\n",
    "model.add(Dense(64)) # create hidden layers which activate data given and gives output (create 64 loops )\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) # does not overfit model\n",
    "model.add(Dense(1)) # all 64 loops are created to be one output\n",
    "model.add(Activation('sigmoid')) # to define or get specific data like 1\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8beb6573-33f9-462f-aedb-48d394b5826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after defining all layers complie as a box  like just provide input and get output\n",
    "model.compile(loss = 'binary_crossentropy',  \n",
    "              optimizer = 'rmsprop',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c019958-ed0d-40bd-8a35-fc75db07089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-28-88a9ca31561c>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.7212 - accuracy: 0.5310 - val_loss: 0.6835 - val_accuracy: 0.5800\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 28s 558ms/step - loss: 0.6898 - accuracy: 0.5370 - val_loss: 0.6758 - val_accuracy: 0.5300\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 25s 498ms/step - loss: 0.6911 - accuracy: 0.5650 - val_loss: 0.7124 - val_accuracy: 0.5300\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 25s 506ms/step - loss: 0.6873 - accuracy: 0.5870 - val_loss: 0.7307 - val_accuracy: 0.5100\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 22s 448ms/step - loss: 0.6579 - accuracy: 0.6140 - val_loss: 0.7152 - val_accuracy: 0.5800\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 22s 441ms/step - loss: 0.6582 - accuracy: 0.6270 - val_loss: 0.6051 - val_accuracy: 0.6900\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.6222 - accuracy: 0.6600 - val_loss: 0.6285 - val_accuracy: 0.6400\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 20s 392ms/step - loss: 0.6214 - accuracy: 0.6630 - val_loss: 0.4890 - val_accuracy: 0.8000\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 19s 379ms/step - loss: 0.6322 - accuracy: 0.6330 - val_loss: 0.5297 - val_accuracy: 0.7200\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 18s 354ms/step - loss: 0.6214 - accuracy: 0.6490 - val_loss: 0.6692 - val_accuracy: 0.6100\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 18s 360ms/step - loss: 0.5929 - accuracy: 0.7010 - val_loss: 0.5631 - val_accuracy: 0.7500\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 17s 343ms/step - loss: 0.5873 - accuracy: 0.6980 - val_loss: 0.6269 - val_accuracy: 0.6300\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 17s 335ms/step - loss: 0.5658 - accuracy: 0.7180 - val_loss: 0.6017 - val_accuracy: 0.6200\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 16s 319ms/step - loss: 0.5869 - accuracy: 0.7100 - val_loss: 0.5912 - val_accuracy: 0.6700\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 16s 330ms/step - loss: 0.5565 - accuracy: 0.7180 - val_loss: 0.5848 - val_accuracy: 0.7200\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 15s 308ms/step - loss: 0.5751 - accuracy: 0.7180 - val_loss: 0.4769 - val_accuracy: 0.7300\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 15s 295ms/step - loss: 0.5494 - accuracy: 0.7290 - val_loss: 0.4562 - val_accuracy: 0.7700\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 14s 285ms/step - loss: 0.5587 - accuracy: 0.7200 - val_loss: 0.5539 - val_accuracy: 0.6900\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 16s 316ms/step - loss: 0.5147 - accuracy: 0.7540 - val_loss: 0.4803 - val_accuracy: 0.7700\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 15s 301ms/step - loss: 0.5400 - accuracy: 0.7350 - val_loss: 0.5972 - val_accuracy: 0.6700\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 15s 310ms/step - loss: 0.5477 - accuracy: 0.7500 - val_loss: 0.4814 - val_accuracy: 0.7800\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 16s 311ms/step - loss: 0.5271 - accuracy: 0.7550 - val_loss: 0.4242 - val_accuracy: 0.8200\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 17s 342ms/step - loss: 0.5232 - accuracy: 0.7580 - val_loss: 0.5050 - val_accuracy: 0.8300\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 17s 347ms/step - loss: 0.5154 - accuracy: 0.7450 - val_loss: 0.5299 - val_accuracy: 0.7400\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 18s 363ms/step - loss: 0.5168 - accuracy: 0.7620 - val_loss: 0.5341 - val_accuracy: 0.7200\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 14s 285ms/step - loss: 0.5341 - accuracy: 0.7440 - val_loss: 0.4665 - val_accuracy: 0.8100\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 14s 275ms/step - loss: 0.5164 - accuracy: 0.7600 - val_loss: 0.6267 - val_accuracy: 0.6900\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 14s 273ms/step - loss: 0.5203 - accuracy: 0.7420 - val_loss: 0.5725 - val_accuracy: 0.7400\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 14s 274ms/step - loss: 0.5308 - accuracy: 0.7460 - val_loss: 0.4979 - val_accuracy: 0.8100\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 13s 261ms/step - loss: 0.5225 - accuracy: 0.7520 - val_loss: 0.4458 - val_accuracy: 0.7900\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 13s 258ms/step - loss: 0.4663 - accuracy: 0.7850 - val_loss: 0.5474 - val_accuracy: 0.7400\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 13s 266ms/step - loss: 0.4702 - accuracy: 0.7800 - val_loss: 0.6158 - val_accuracy: 0.6600\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 14s 275ms/step - loss: 0.5091 - accuracy: 0.7660 - val_loss: 0.4850 - val_accuracy: 0.7600\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 14s 283ms/step - loss: 0.5040 - accuracy: 0.7590 - val_loss: 0.5054 - val_accuracy: 0.7900\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 14s 271ms/step - loss: 0.5181 - accuracy: 0.7590 - val_loss: 0.5104 - val_accuracy: 0.7600\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 14s 279ms/step - loss: 0.5035 - accuracy: 0.7690 - val_loss: 0.4494 - val_accuracy: 0.8200\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 14s 276ms/step - loss: 0.5054 - accuracy: 0.7690 - val_loss: 0.5532 - val_accuracy: 0.7000\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 13s 266ms/step - loss: 0.5253 - accuracy: 0.7700 - val_loss: 0.4640 - val_accuracy: 0.7800\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 13s 260ms/step - loss: 0.4897 - accuracy: 0.7620 - val_loss: 0.4513 - val_accuracy: 0.8400\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 13s 267ms/step - loss: 0.5044 - accuracy: 0.7630 - val_loss: 0.5047 - val_accuracy: 0.7900\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 14s 273ms/step - loss: 0.4626 - accuracy: 0.7860 - val_loss: 0.5323 - val_accuracy: 0.7500\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 14s 282ms/step - loss: 0.4637 - accuracy: 0.7960 - val_loss: 0.3831 - val_accuracy: 0.8300\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 15s 292ms/step - loss: 0.4819 - accuracy: 0.7780 - val_loss: 0.4807 - val_accuracy: 0.7900\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 14s 271ms/step - loss: 0.5019 - accuracy: 0.7550 - val_loss: 0.3790 - val_accuracy: 0.7800\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 14s 274ms/step - loss: 0.4962 - accuracy: 0.7750 - val_loss: 0.4095 - val_accuracy: 0.8100\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 13s 268ms/step - loss: 0.4681 - accuracy: 0.7790 - val_loss: 0.4799 - val_accuracy: 0.8100\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 13s 257ms/step - loss: 0.5025 - accuracy: 0.7720 - val_loss: 0.4416 - val_accuracy: 0.7900\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 13s 265ms/step - loss: 0.4730 - accuracy: 0.7880 - val_loss: 0.4192 - val_accuracy: 0.8300\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 14s 274ms/step - loss: 0.4856 - accuracy: 0.7850 - val_loss: 0.3820 - val_accuracy: 0.8300\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 14s 276ms/step - loss: 0.4889 - accuracy: 0.7670 - val_loss: 0.3401 - val_accuracy: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cd08b72bb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the augmentation configuration we use for training\n",
    "model.fit_generator(\n",
    "      train_generate,\n",
    "      steps_per_epoch = num_train_samples // batch_size,\n",
    "      epochs = epochs,\n",
    "      validation_data = test_generate,\n",
    "      validation_steps = num_test_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a335b960-3bcf-4d84-bcf8-119804c31921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to perform the model in hardware or any prediction\n",
    "model.save_weights('D:/Cats and Dogs/dataset/model2/trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26ddd9c3-3b38-4426-b27b-014d2967aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image for prediction from test data\n",
    "img_pred = image.load_img('D:/Cats and Dogs/dataset/test_set/dogs/dog.4002.jpg', target_size= (150,150)) # loading images (specfying a cat image from test data) and target size of 150 x 150 \n",
    "img_pred = image.img_to_array(img_pred) # image array : convert of image to array type\n",
    "img_pred = np.expand_dims(img_pred, axis = 0) # expand dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb6043f3-6ec0-4c78-94b4-1c42c3ce4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model weights to perform the model in hardware or any prediction\n",
    "model.load_weights('D:/Cats and Dogs/dataset/model2/trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a9d9656-7b40-4e53-9874-0e06f8dfb653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "# prediction of result\n",
    "result = model.predict(img_pred) # predict the array img using model\n",
    "print(result) # print result\n",
    "if result[0][0] == 0: # if given 2d array is 0 then it is dog else cat\n",
    "    prediction = \"dog\"\n",
    "else:\n",
    "    prediction = \"cat\"\n",
    "    \n",
    "print (prediction)  # print prediction of images   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50615df3-2a9a-4bdf-aa87-cc4dd4b2dfd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
